from dataclasses import dataclass, field
from typing import Optional
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import Trainer
from transformers import TrainingArguments as HFTrainingArguments

@dataclass
class ModelArguments:
    """
    Arguments for configuring and loading the model and tokenizer used in training.
    """
    model_name_or_path: str = field(
        default="bigcode/starcoder2-3b",
        metadata={"help": "Path or name of the pre-trained model to fine-tune."}
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={"help": "Path or name of the tokenizer to use. Defaults to model_name_or_path."}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Path to a directory where the model/tokenizer cache should be stored."}
    )

    def load_model_and_tokenizer(self):
        """
        Load the model and tokenizer based on the provided paths or names.
        """
        model_path = self.model_name_or_path
        tokenizer_path = self.tokenizer_name_or_path or model_path

        print(f"Loading model from: {model_path}")
        print(f"Loading tokenizer from: {tokenizer_path}")

        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, cache_dir=self.cache_dir)   #if the architecture changes this line must be changed
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path, revision="main")   #if the architecture changes this line must be changed

        return model, tokenizer
    
@dataclass
class TrainingArguments(HFTrainingArguments):
    """
    Training arguments tailored for Salesforce/codet5p-770m or similar encoder-decoder models.
    """
    '''cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Directory for caching pre-trained models and tokenizers."}
    )'''
    optim: str = field(
        default="adamw_torch",
        metadata={"help": "Optimizer to use during training. Default is AdamW implemented in PyTorch."}
    )
    '''model_max_length: int = field(
        default=512,
        metadata={"help": "Maximum sequence length. Sequences longer than this will be truncated."}
    )'''
    learning_rate: float = field(
        default=5e-5,
        metadata={"help": "Learning rate for training."}
    )
    warmup_steps: int = field(
        default=0,
        metadata={"help": "Number of warmup steps for the learning rate scheduler."}
    )
    weight_decay: float = field(
        default=0.01,
        metadata={"help": "Weight decay for the AdamW optimizer."}
    )
    per_device_train_batch_size: int = field(
        default=16,
        metadata={"help": "Batch size per device for training."}
    )
    per_device_eval_batch_size: int = field(
        default=16,
        metadata={"help": "Batch size per device for evaluation."}
    )
    eval_strategy: str = field(
        default="steps",
        metadata={"help": "Evaluation strategy to use. Choose from 'no', 'steps', or 'epoch'."}
    )
    save_steps: int = field(
        default=500,
        metadata={"help": "Number of steps between saving checkpoints."}
    )
    logging_steps: int = field(
        default=50,
        metadata={"help": "Number of steps between logging outputs."}
    )
    '''predict_with_generate: bool = field(
        default=True,
        metadata={
            "help": "Whether to use `generate()` to compute metrics during evaluation for sequence-to-sequence tasks."
        }
    )'''
    fp16: bool = field(
        default=True,
        metadata={"help": "Use 16-bit (mixed) precision instead of 32-bit."}
    )

class DynamicTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        # Initialize the custom trainer by calling the parent class constructor
        super(DynamicTrainer, self).__init__(*args, **kwargs)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        Compute the loss by calculating the Cross-Entropy Loss.
        The loss will be automatically generated by the model using the provided labels.
        
        Args:
        - model: The neural network model to generate predictions.
        - inputs: A dictionary containing the input and labels.
        - return_outputs (optional): Flag indicating whether to return outputs along with loss.
        
        Returns:
        - total_loss: The final calculated loss used for backpropagation.
        - outputs: (optional) model outputs returned if `return_outputs` is set to True.
        """
        labels = inputs.get("labels")  # Get the true labels from the inputs
        input_ids = inputs.get("input_ids")  # Get the input tokens to feed to the model

        # Pass input_ids to the model and calculate loss based on labels (Cross-Entropy Loss)
        outputs = model(input_ids=input_ids,
                        attention_mask=inputs.get("attention_mask"),
                        labels=labels)  # 'labels' are used to calculate the loss automatically

        # If the flag is set, return both the loss and the outputs (e.g., for tracking or further processing)
        if return_outputs:
            return outputs.loss, outputs
        return outputs.loss  # Return just the computed total loss for backpropagation

